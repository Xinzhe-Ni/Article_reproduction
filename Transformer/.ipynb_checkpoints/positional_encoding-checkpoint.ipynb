{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44ac636",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fixed Positional Encodings\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from labml_helpers.module import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea638fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_encoding(d_model: int, max_len: int = 5000):\n",
    "    encodings = torch.zeros(max_len, d_model)\n",
    "    position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)   # 位置索引\n",
    "    two_i = torch.arange(0, d_model, 2, dtype=torch.float32)\n",
    "    div_term = torch.exp(two_i * -(math.log(10000.0) / d_model))\n",
    "    encodings[:, 0::2] = torch.sin(position * div_term)\n",
    "    encodings[:, 1::2] = torch.cos(position * div_term)\n",
    "    encodings = encodings.unsqueeze(1).requires_grad_(False)   # 加入batch的维度\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76e9218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test_positional_encoding():\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    pe = get_positional_encoding(20, 100)\n",
    "    plt.plot(np.arange(100), pe[:, 0, 4:8].numpy())\n",
    "    plt.legend([\"dim %d\" % p for p in [4, 5, 6, 7]])\n",
    "    plt.title(\"Positional encoding\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':   # 只有直接运行该脚本才会被执行，被import时不执行\n",
    "    _test_positional_encoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ee111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(Module):\n",
    "    \n",
    "    def __init__(self, d_model: int, dropout_prob: float, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        self.register_buffer('positional_encodings', get_positional_encoding(d_model, max_len), False)   # 该参数在模型训练时不会被更新\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):   # x形状为（seq_len, batch_size, d_model）\n",
    "        pe = self.positional_encodings[:x.shape[0]].detach().requires_grad_(False)\n",
    "        x = x + pe\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
