{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539fc2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transformer Encoder and Decoder Models\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from labml_helpers.module import Module\n",
    "\n",
    "from labml_nn.utils import clone_module_list\n",
    "from feed_forward import FeedForward\n",
    "from mha import MultiHeadAttention\n",
    "from positional_encoding import get_positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1bdaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsWithPositionalEncoding(Module):   # 使用文章中固定的位置编码\n",
    "    \n",
    "    def __init__(self, d_model: int, n_vocab: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Embedding(n_vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.register_buffer('positional_encodings', get_positional_encoding(d_model, max_len))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        pe = self.positional_encodings[:x.shape[0]].requires_grad_(False)\n",
    "        return self.linear(x) * math.sqrt(self.d_model) + pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee673ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsWithLearnedPositionalEncoding(Module):   # 使用可学习的位置编码\n",
    "    \n",
    "    def __init__(self, d_model: int, n_vocab: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Embedding(n_vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.positional_encodings = nn.Parameter(torch.zeros(max_len, 1, d_model), requires_grad=True)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        pe = self.positional_encodings[:x.shape[0]]\n",
    "        return self.linear(x) * math.sqrt(self.d_model) + pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b3cea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(Module):\n",
    "    \n",
    "    def __init__(self, *,\n",
    "                 d_model: int,\n",
    "                 self_attn: MultiHeadAttention,\n",
    "                 src_attn: MultiHeadAttention = None,\n",
    "                 feed_forward: FeedForward,\n",
    "                 dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.size = d_model\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.norm_self_attn = nn.LayerNorm([d_model])\n",
    "        if self.src_attn is not None:\n",
    "            self.norm_src_attn = nn.LayerNorm([d_model])\n",
    "        self.norm_ff = nn.LayerNorm([d_model])\n",
    "        self.is_save_ff_input = False   # 是否将输入保存到前馈层\n",
    "    \n",
    "    def forward(self, *,\n",
    "                x: torch.Tensor,\n",
    "                mask: torch.Tensor,\n",
    "                src: torch.Tensor = None,\n",
    "                src_mask: torch.Tensor = None):\n",
    "        z = self.norm_self_attn(x)\n",
    "        self_attn = self.self_attn(query=z, key=z, value=z, mask=mask)\n",
    "        x = x + self.dropout(self_attn)   # 残差连接\n",
    "        if src is not None:\n",
    "            z = self.norm_src_attn(x)\n",
    "            attn_src = self.src_attn(query=z, key=src, value=src, mask=src_mask)   # 此处q和k应该是src，v是z？\n",
    "            x = x + self.dropout(attn_src)\n",
    "        z = self.norm_ff(x)\n",
    "        if self.is_save_ff_input:\n",
    "            self.ff_input = z.clone()\n",
    "        ff = self.feed_forward(z)\n",
    "        x = x + self.dropout(ff)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f5a744",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Module):\n",
    "    \n",
    "    def __init__(self, layer: TransformerLayer, n_layers: int):\n",
    "        super().__init__()\n",
    "        self.layers = clone_module_list(layer, n_layers)   #复制多个Transformer块\n",
    "        self.norm = nn.LayerNorm([layer.size])\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x=x, mask=mask)\n",
    "        return self.norm(x)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812e56cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Module):\n",
    "    \n",
    "    def __init__(self, layer: TransformerLayer, n_layers: int):\n",
    "        super().__init__()\n",
    "        self.layers = clone_module_list(layer, n_layers)\n",
    "        self.norm = nn.LayerNorm([layer.size])\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, memory: torch.Tensor, src_mask: torch.Tensor, tgt_mask: torch.Tensor):   #src指编码器输入，tgt指解码器输入\n",
    "        for layer in self.layers:\n",
    "            x = layer(x=x, mask=tgt_mask, src=memory, src_mask=src_mask)\n",
    "        return self.norm(x)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204230c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(Module):   # 预测token\n",
    "    \n",
    "    def __init__(self, n_vocab: int, d_model: int):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(d_model, n_vocab)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.projection(x)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038961fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(Module):\n",
    "    \n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: Module, tgt_embed: Module, generator: Module):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)   # 参数初始化\n",
    "                \n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor, src_mask: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        enc = self.encode(src, src_mask)\n",
    "        return self.decode(enc, src_mask, tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src: torch.Tensor, src_mask: torch.Tensor):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
